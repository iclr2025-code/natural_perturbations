# prompt openai models (without gpus needed)
import subprocess
subprocess.run(["pip", "install", "openai"]) # install openai package

import openai
openai.api_key = "key" # replace with your own openai api key
import time

# a reading comprehension example to be input to the model
instruction = '''read the context and answer the question.'''+'\n\n'+'''context: Southern California, often abbreviated SoCal, is a geographic and cultural region that generally comprises California's southernmost 10 counties. The region is traditionally described as "eight counties", based on demographics and economic ties: Imperial, Los Angeles, Orange, Riverside, San Bernardino, San Diego, Santa Barbara, and Ventura. The more extensive 10-county definition, including Kern and San Luis Obispo counties, is also used based on historical political divisions. Southern California is a major economic center for the state of California and the United States.'''+'\n\n'+'''question: What is Southern California often abbreviated as?'''

try:
  response = openai.chat.completions.create(
      model="gpt-4o", # play with any openai models (https://platform.openai.com/docs/models)
      messages=[
          {"role": "user", "content": instruction}
      ]
  )
  answer = response.choices[0].message.content # response/output generated by the model
  print(answer)
  time.sleep(3)

except openai.error.ServiceUnavailableError:
  print("Service Unavailable. Retrying in 2 minutes...")
  time.sleep(120)

# prompt other large language models (at least 1 A100 GPU is needed (preferably two))
TOKEN = 'token'

subprocess.run(["huggingface-cli", "login", "--token", TOKEN])

subprocess.run(['pip', 'install', 'transformers', '--upgrade'])
subprocess.run(['pip', 'install', 'accelerate', '--upgrade'])

import transformers
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# load google's gemma model https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b
tokenizer_gemmatb = AutoTokenizer.from_pretrained("google/gemma-2b-it")
model_gemmatb = AutoModelForCausalLM.from_pretrained("google/gemma-2b-it", device_map="auto")
tokenizer_gemmasb = AutoTokenizer.from_pretrained("google/gemma-7b-it")
model_gemmasb = AutoModelForCausalLM.from_pretrained("google/gemma-7b-it", device_map="auto")

def gemmatb(instruction):
  input_ids = tokenizer_gemmatb(instruction, return_tensors="pt").to("cuda")
  outputs = model_gemmatb.generate(**input_ids, max_new_tokens=2000)
  return tokenizer_gemmatb.decode(outputs[0])

def gemmasb(instruction):
  input_ids = tokenizer_gemmasb(instruction, return_tensors="pt").to("cuda")
  outputs = model_gemmasb.generate(**input_ids, max_new_tokens=2000)
  return tokenizer_gemmasb.decode(outputs[0])

# load meta's llama2 https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b
tokenizer_llamasb = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model_llamasb = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", device_map="auto")
tokenizer_llamathirteenb = AutoTokenizer.from_pretrained("meta-llama/Llama-2-13b-chat-hf")
model_llamathirteenb = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b-chat-hf", device_map="auto")

generate_text_pipeline_llamasb = transformers.pipeline(
    model=model_llamasb, tokenizer=tokenizer_llamasb,
    return_full_text=False,
    task="text-generation",

    temperature=0.1,
    max_new_tokens=1000,
    repetition_penalty=1.1
)

generate_text_pipeline_llamathirteenb = transformers.pipeline(
    model=model_llamathirteenb, tokenizer=tokenizer_llamathirteenb,
    return_full_text=False,
    task="text-generation",

    temperature=0.1,
    max_new_tokens=1000,
    repetition_penalty=1.1
)

def llamasb(instruction):
  res = generate_text_pipeline_llamasb(instruction)
  return res[0]["generated_text"]

def llamathirteenb(instruction):
  res = generate_text_pipeline_llamathirteenb(instruction)
  return res[0]["generated_text"]

# mistral model https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
tokenizer_Mistralsb = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
model_Mistralsb = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", device_map="auto")

def Mistralsb(instruction):
  messages = [
          {"role": "user", "content": instruction}
  ]
  inputs = tokenizer_Mistralsb.apply_chat_template(messages, return_tensors="pt").to("cuda")
  outputs = model_Mistralsb.generate(inputs, max_new_tokens=1000)
  returned = tokenizer_Mistralsb.decode(outputs[0], skip_special_tokens=True)
  return returned.split("[/INST] ")[1]

# load facon models https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a
model_falconsb = "tiiuae/falcon-7b-instruct"
tokenizer_falconsb = AutoTokenizer.from_pretrained(model_falconsb)
pipeline_falconsb = transformers.pipeline(
    "text-generation",
    model=model_falconsb,
    tokenizer=tokenizer_falconsb,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)
model_falconfortyb = "tiiuae/falcon-40b-instruct"
tokenizer_falconfortyb = AutoTokenizer.from_pretrained(model_falconfortyb)
pipeline_falconfortyb = transformers.pipeline(
    "text-generation",
    model=model_falconfortyb,
    tokenizer=tokenizer_falconfortyb,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
)

def falconsb(instruction):
  sequences = pipeline_falconsb(
            instruction,
            max_length=2000,
            do_sample=True,
            top_k=10,
            num_return_sequences=1,
            eos_token_id=tokenizer_falconsb.eos_token_id,
        )
  return sequences[0]['generated_text']

def falconfortyb(instruction):
  sequences = pipeline_falconfortyb(
            instruction,
            max_length=2000,
            do_sample=True,
            top_k=10,
            num_return_sequences=1,
            eos_token_id=tokenizer_falconfortyb.eos_token_id,
  )
  return sequences[0]['generated_text']

# all models take the instruction (e.g., our pre-defined reading comprehension example) as input and output the answer, though with a bit difference in instruction format for differnt models
# for gemma and facon
response = falconfortyb(instruction+"Answer:")
# for llama2
response = llamasb("[INST] "+instruction+" [/INST]")
# for mistral
response = Mistralsb(instruction)